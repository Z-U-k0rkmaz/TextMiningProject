{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  writer  file_id                                             column\n",
      "0    ttu       27  O zaman 28 Şubat neden yapıldı?  \\r\\n     \\r\\n...\n",
      "1    ttu       28  O zaman 28 Şubat neden yapıldı?  \\r\\n     \\r\\n...\n",
      "2    ttu        3  Cumhuriyetin en kritik yerel seçimi \\r\\nMURAT ...\n",
      "3    ttu        4  Çekilin artık kamera karşısından \\r\\nHER kar y...\n",
      "4    ttu       23  Lorant'ın heyecan veren hedefi  \\r\\n     \\r\\n\\...\n",
      "5    ttu       14  Gerçekten de gülüp geçilecek bir öneri \\r\\nGAZ...\n",
      "6    ttu       13  Mercedes çeşitlemeleri \\r\\nTÜRK insanı, Merced...\n",
      "7    ttu       31  Yazarın çilesi  \\r\\n     \\r\\n\\r\\n  \\r\\nZAMAN z...\n",
      "8    ttu       19  Tüm yazı konularını silip süpüren fotoğraf \\r\\...\n",
      "9    ttu       11  Şarkta böyledir bu işler...  \\r\\n     \\r\\nttu...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 630 entries, 0 to 629\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   writer   630 non-null    object\n",
      " 1   file_id  630 non-null    int64 \n",
      " 2   column   630 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 14.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_json(\"dataset\\Turkish_data_depository_630koseyazisi.jsonl\",lines=True)\n",
    "print(dataset.head(10))\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type x : <class 'numpy.ndarray'> , type y : <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = dataset.iloc[:, 2].values.astype(\"str\") # author text\n",
    "y = dataset.iloc[:, 0].values.astype(\"str\") # author name\n",
    "\n",
    "print(f\"type x : {type(x)} , type y : {type(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =load_stop_words(\"stopwordsTR\\stopwords.txt\") #set(stopwords.words('turkish'))\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "# Lemmatizasyon ve stemming işlemleri için hazırlık\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "# Her bir metni lemmatizasyon ve stemming işlemlerinden geçirme\n",
    "for i in range(len(x)):\n",
    "    # Convert to lowercase - Küçük harfe çevirme\n",
    "    x[i] = x[i].lower()\n",
    "    \n",
    "    # Remove escape characters - Kaçış karakterlerini kaldırma\n",
    "    x[i] = re.sub(r'[\\r\\n]', ' ', x[i]) \n",
    "    \n",
    "    # Remove unnecessary characters - Gereksiz karakterleri kaldırma\n",
    "    x[i] = re.sub(r'[^a-zA-ZğüşıöçĞÜŞİÖÇ\\s]', '', x[i])\n",
    "    \n",
    "    # Tokenization - Kelime ayırma\n",
    "    tokens = word_tokenize(x[i])\n",
    "    \n",
    "    # Remove stop words - Stop kelimeleri kaldırma\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    x[i] = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    x[i] = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    # Remove punctuation marks - Noktalama işaretlerini kaldırma\n",
    "    x[i] = x[i].translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove repeating spaces - Tekrar eden boşlukları kaldırma\n",
    "    x[i] = re.sub(r'\\s+', ' ', x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X verisini unigram TF-IDF vektörlerine dönüştürme\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X_vec = vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# SGDClassifier için hiperparametrelerin belirlenmesi\n",
    "param_grid = {\n",
    "    'alpha': [1e-05, 0.0001, 0.001],\n",
    "    'max_iter': [50, 100, 200],\n",
    "    'penalty': ['elasticnet', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'alpha': 0.001, 'max_iter': 50, 'penalty': 'l2'}\n",
      "En iyi skor: 0.7816633663366337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Umitk\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Umitk\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Umitk\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Umitk\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Umitk\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Umitk\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Çapraz doğrulama skorları: [0.46153846 0.6        0.64       0.72       0.44      ]\n",
      "Ortalama çapraz doğrulama skoru: 0.5723076923076923\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV ile model oluşturma\n",
    "grid_search = GridSearchCV(SGDClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# En iyi parametreleri ve skorları gösterme\n",
    "print(\"En iyi parametreler:\", grid_search.best_params_)\n",
    "print(\"En iyi skor:\", grid_search.best_score_)\n",
    "\n",
    "# Çapraz doğrulama ile model performansını değerlendirme\n",
    "cross_val_scores = cross_val_score(grid_search, X_test, y_test, cv=5)\n",
    "print(\"Çapraz doğrulama skorları:\", cross_val_scores)\n",
    "print(\"Ortalama çapraz doğrulama skoru:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7063\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier ile model oluşturma\n",
    "model = SGDClassifier(alpha=1e-05, max_iter=50, penalty='elasticnet', random_state=42)\n",
    "\n",
    "# Modeli eğitme\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin performansını değerlendirme\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
